{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iMH4cIE86FRi"},"outputs":[],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBTlwAz07qvf"},"outputs":[],"source":["!pip3 install tokenizers sentencepiece #wandb"]},{"cell_type":"code","source":["# !pip install transformers\n","!pip install git+https://github.com/llohann-speranca/transformers.git@fix-resume-checkpoint-for-peftmodel"],"metadata":{"id":"9VTPdEHFI8HG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"df_3VUzL7vEB"},"outputs":[],"source":["!pip3 install huggingface-hub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gsxwyby7AUgC"},"outputs":[],"source":["!pip3 install datasets peft trl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GxXUgy_g7qlK"},"outputs":[],"source":["!pip3 install bertviz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwGsbk0KDHaz"},"outputs":[],"source":["!pip install accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_E_lJ2AOCsyL"},"outputs":[],"source":["!pip install bitsandbytes einops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D613nlta7wzS"},"outputs":[],"source":["import os\n","os.chdir(\"drive/\")\n","os.chdir('My Drive')\n","os.chdir('Experiment')\n","os.chdir('LLMs')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaCLZWGL7zzs"},"outputs":[],"source":["OUTPUT_DIR = './llama2-outputs/'\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2MZpfIR8ZUM"},"outputs":[],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","source":["# from huggingface_hub import notebook_login\n","\n","# notebook_login()"],"metadata":{"id":"QmfZtrtODKqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9D_znaHL8FFA"},"outputs":[],"source":["from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import torch\n","from datasets import load_dataset\n","from peft import LoraConfig\n","from peft import PeftModel, PeftConfig\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft.tuners.lora import LoraLayer\n","\n","from trl import SFTTrainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fXH_TdBo8FCB"},"outputs":[],"source":["########################################################################\n","# This is a fully working simple example to use trl's RewardTrainer.\n","#\n","# This example fine-tunes any causal language model (GPT-2, GPT-Neo, etc.)\n","# by using the RewardTrainer from trl, we will leverage PEFT library to finetune\n","# adapters on the model.\n","#\n","########################################################################"]},{"cell_type":"markdown","metadata":{"id":"Q-m_Zy2R8SIE"},"source":["## Configs and Arguments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOa19W6i8E_Z"},"outputs":[],"source":["class Arguments:\n","    local_rank:int = -1\n","    per_device_train_batch_size = 8\n","    per_device_eval_batch_size = 1\n","\n","    learning_rate = 2e-4\n","    max_grad_norm = 0.3\n","    weight_decay = 0.001\n","\n","    lora_alpha = 16\n","    lora_dropout = 0.1\n","    lora_r = 64\n","    max_seq_length = 1024 #768 #512\n","\n","    model_name = \"NousResearch/Llama-2-7b-hf\" # 'meta-llama/Llama-2-7b-chat-hf', 'NousResearch/Llama-2-13b-hf'\n","    dataset_name = \"databricks/databricks-dolly-15k\" # \"mlabonne/guanaco-llama2-1k\", \"databricks/databricks-dolly-15k\"\n","\n","    new_model = \"Llama-2-7b-dolly\"\n","\n","    use_4bit = True\n","    use_nested_quant = False\n","    bnb_4bit_compute_dtype = \"float16\"\n","    bnb_4bit_quant_type = \"nf4\"\n","\n","    num_train_epochs = 1\n","\n","    fp16 = False\n","    bf16 = True\n","\n","    gradient_accumulation_steps = 1\n","    packing = False\n","    gradient_checkpointing = True\n","    optim = \"paged_adamw_32bit\"\n","    lr_scheduler_type = \"constant\" # Constant a bit better than cosine, and has advantage for analysis\n","\n","    max_steps: int = 10000\n","    warmup_ratio: float = 0.03\n","    group_by_length: bool = True # \"Group sequences into batches with same length. Saves memory and speeds up training considerably.\"\n","\n","    save_steps: int = 100\n","    logging_steps: int = 100\n","\n","\n","script_args = Arguments()"]},{"cell_type":"code","source":["# Load the entire model on the GPU 0\n","device_map = {\"\": 0}\n"],"metadata":{"id":"_UlSXJcaC9U_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mAhJMJlD8YpR"},"source":["## Model"]},{"cell_type":"code","source":["from torch.nn import functional as F\n","\n","\n","# <https://github.com/huggingface/trl/issues/870>\n","# <https://github.com/neelsjain/NEFTune#code>\n","def NEFTune(model, noise_alpha=5)\n","    def noised_embed(orig_embed, noise_alpha):\n","        def new_func(x):\n","            # during training, we add noise to the embedding\n","            # during generation, we don't add noise to the embedding\n","            if model.training:\n","                embed_init = orig_embed(x)\n","                dims = torch.tensor(embed_init.size(1) * embed_init.size(2))\n","                mag_norm = noise_alpha/torch.sqrt(dims)\n","                return embed_init + torch.zeros_like(embed_init).uniform_(-mag_norm, mag_norm)\n","            else:\n","                return orig_embed(x)\n","        return new_func\n","    ##### NOTE: this is for a LLaMA model #####\n","    ##### For a different model, you need to change the attribute path to the embedding #####\n","    model.base_model.model.model.embed_tokens.forward = noised_embed(model.base_model.model.model.embed_tokens, noise_alpha)\n","    return model"],"metadata":{"id":"RrIyBtGGXam2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9a8nATE8E9O"},"outputs":[],"source":["def create_and_prepare_model(args):\n","    compute_dtype = getattr(torch, args.bnb_4bit_compute_dtype)\n","\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=args.use_4bit,\n","        bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n","        bnb_4bit_compute_dtype=compute_dtype,\n","        bnb_4bit_use_double_quant=args.use_nested_quant,\n","    )\n","\n","    if compute_dtype == torch.float16 and args.use_4bit:\n","        major, _ = torch.cuda.get_device_capability()\n","        if major >= 8:\n","            print(\"=\" * 80)\n","            print(\"Your GPU supports bfloat16, you can accelerate training with the argument --bf16\")\n","            print(\"=\" * 80)\n","\n","    device_map = {\"\": 0}\n","\n","    model = AutoModelForCausalLM.from_pretrained(\n","        args.model_name, quantization_config=bnb_config, device_map=device_map, trust_remote_code=True\n","    )\n","\n","    peft_config = LoraConfig(\n","        lora_alpha=script_args.lora_alpha,\n","        lora_dropout=script_args.lora_dropout,\n","        r=script_args.lora_r,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","        target_modules=[\n","            'q_proj',\n","            'k_proj',\n","            'v_proj',\n","            'o_proj',\n","            'gate_proj',\n","            'up_proj',\n","            'down_proj',\n","        ]\n","        # target_modules=[\n","        #     \"query_key_value\",\n","        #     \"dense\",\n","        #     \"dense_h_to_4h\",\n","        #     \"dense_4h_to_h\",\n","        # ],  # , \"word_embeddings\", \"lm_head\"],\n","    )\n","\n","    tokenizer = AutoTokenizer.from_pretrained(script_args.model_name, trust_remote_code=True)\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","    if args.gradient_checkpointing:\n","        model.gradient_checkpointing_enable()\n","\n","    return model, peft_config, tokenizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUas6pzY8E7J"},"outputs":[],"source":["training_arguments = TrainingArguments(\n","    output_dir=OUTPUT_DIR,\n","    per_device_train_batch_size=script_args.per_device_train_batch_size,\n","    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n","    optim=script_args.optim,\n","    save_steps=script_args.save_steps,\n","    logging_steps=script_args.logging_steps,\n","    learning_rate=script_args.learning_rate,\n","    fp16=script_args.fp16,\n","    bf16=script_args.bf16,\n","    max_grad_norm=script_args.max_grad_norm,\n","    max_steps=script_args.max_steps,\n","    warmup_ratio=script_args.warmup_ratio,\n","    group_by_length=script_args.group_by_length,\n","    lr_scheduler_type=script_args.lr_scheduler_type,\n",")\n","\n","\n","model, peft_config, tokenizer = create_and_prepare_model(script_args)\n","model.config.use_cache = False\n","dataset = load_dataset(script_args.dataset_name, split=\"train\")"]},{"cell_type":"markdown","metadata":{"id":"2RATtfRJ8lPw"},"source":["## Training"]},{"cell_type":"code","source":["model"],"metadata":{"id":"B-ZfVrQhXdNe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[0]"],"metadata":{"id":"rFViXK1Y6ETE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def formatting_prompts_func(example):\n","    output_texts = []\n","    for i in range(len(example['instruction'])):\n","        instruction = example['instruction'][i]\n","        input_context = example['context'][i]\n","        response = example['response'][i]\n","\n","        text = PROMPT_WITH_INPUT_FORMAT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction\n","{0}\n","\n","Input:\n","{1}\n","\n","### Response:\n","{2}\n","\n","### End\"\"\".format(instruction, input_context, response)\n","        # text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n","        output_texts.append(text)\n","    return output_texts"],"metadata":{"id":"BPdbop5JMsRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojVwtusY8E4-"},"outputs":[],"source":["# trainer = SFTTrainer(\n","#     model=model,\n","#     train_dataset=dataset,\n","#     peft_config=peft_config,\n","#     dataset_text_field=\"text\",\n","#     max_seq_length=script_args.max_seq_length,\n","#     tokenizer=tokenizer,\n","#     args=training_arguments,\n","#     packing=script_args.packing,\n","# )\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    # dataset_text_field=\"text\",\n","    formatting_func=formatting_prompts_func,\n","    max_seq_length=script_args.max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=script_args.packing,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUkfm9xs8E1r"},"outputs":[],"source":["for name, module in trainer.model.named_modules():\n","    if isinstance(module, LoraLayer):\n","        if script_args.bf16:\n","            module = module.to(torch.bfloat16)\n","    if \"norm\" in name:\n","        module = module.to(torch.float32)\n","    if \"lm_head\" in name or \"embed_tokens\" in name:\n","        if hasattr(module, \"weight\"):\n","            if script_args.bf16 and module.weight.dtype == torch.float32:\n","                module = module.to(torch.bfloat16)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rkEUw058Ex4"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eUPhOnXg8Esa"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyMWZ2Z88XWl2U1mwF3iCzlL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}